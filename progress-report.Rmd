---
title: "Project2-DataScience"
author: "Abanoub Mimi, Amir George, Tony Foti"
date: "May 8, 2016"
output: html_document
---

#Exploratory analysis (Amir)
We begin by attaching required libraries and loading the datasets and relevant classifiers.
```{r message=FALSE}
library(dplyr)
library(knitr)
library(RWeka)
library(tidyr)
```
```{r cache=TRUE}
trainDf <- read.csv('data/train.csv')
testDf <- read.csv('data/test.csv')
contractRefDf <- read.csv('data/contract_ref.csv')
calendarRefDf <- read.csv('data/calendar_ref.csv')
dailyAggDf <- read.csv('data/daily_aggregate.csv')
roamingDf <- read.csv('data/roaming_monthly.csv')
```
```{r}
trainDf$TARGET <- as.factor(trainDf$TARGET)
RF <- make_Weka_classifier("weka/classifiers/trees/RandomForest")
NB <- make_Weka_classifier("weka/classifiers/bayes/NaiveBayes")
MLP <- make_Weka_classifier("weka/classifiers/functions/MultilayerPerceptron")
```
## Train and Test datasets
We proceed by taking a look at the training and test datasets:
```{r}
trainDf %>% dim
trainDf$CONTRACT_KEY %>% unique %>% length
trainDf %>% summary
sapply(trainDf, function(x) sum(is.na(x)))
testDf %>% dim
testDf$CONTRACT_KEY %>% unique %>% length
testDf %>% summary
sapply(testDf, function(x) sum(is.na(x)))
```
There are no NA values in the original train and test datasets, and no multiple observations for the same `CONTRACT_KEY`.

## ContractRef dataset
We now proceed with examiming the `contract_ref` dataset.
```{r}
contractRefDf %>% dim
contractRefDf$CONTRACT_KEY %>% unique %>% length
```
No multiple observations for the same `CONTRACT_KEY`.
```{r}
sapply(contractRefDf, function(x) sum(is.na(x)))
contractRefDf %>% summary
```
Some weird observations are spotted in this examination. There are negative age values. There are age values at 99 which are highly unlikely. The gender has missing values represented by the levels `Not Entered` and `Unknown`. Gender has duplicate values denoting males which are `Male` and `M`. Same for `Female` and `F`. There are 6 NA observations in the `VALUE_SEGMENT` column, as well as 8169 values of "N/A" which are basically the same as NA values.
### Unnaturally small age values
```{r}
contractRefDf[contractRefDf$AGE < 15,]
```
There are 5 observations with a negative age, which is impossible. Also, there are 7 observations with suspiciously small ages ranging from 1 to 4 years.

### Age values of `99`
```{r}
contractRefDf[contractRefDf$AGE == 99,] %>% nrow
```
Almost half the observations in this dataset (51263 out of 107743) have an age value of `99`, which could be a default value for unknown or not entered ages.

###Incomplete `GENDER` observations
More than half the oservations (54379 out of 107743) have missing gender values labeled as `Not Entered`, while 3148 observations have gender values labeled as `Unknown`.
```{r}
length(intersect(unique(trainDf$CONTRACT_KEY),unique(contractRefDf[contractRefDf$GENDER == "Not Entered",]$CONTRACT_KEY)))
length(intersect(unique(testDf$CONTRACT_KEY),unique(contractRefDf[contractRefDf$GENDER == "Not Entered",]$CONTRACT_KEY)))
```
The missing observations are not tied to customers of train dataset only or test dataset only as shown above.
```{r}
contractRefDf$GENDER %>% unique
```
Moreover, the male gender has multiple labels (`Male`, `M` & `m`), as well as the female gender (`Female`, `F` & `f`).

###NAs in `VALUE_SEGMENT`
There are 6 NA observations in the `VALUE_SEGMENT` column, as well as 8169 values of "N/A" which are basically the same as NA values. So, these are all missing values.


#Progress tracking
In the following subsections we present a brief description of several approaches we tried in order to increase the score. Each subsection denotes the person who carried out its approach, and related files to this approach (whether they are scripts, output files or workspace images) can be found in a folder inside the `scripts` folder with the following naming convention: `Number-Implementer-Score`. The code chunks embedded in this section are for illustration purposes only.

##4- Decisioon Tree with merging all `contract_ref.csv` columns (Amir)
By continuing on number `1` above (Decision Tree classifier), I merged with all columns from `contract_ref.csv`, and obtained a score of `0.57585`, which is a decrease from the original score, so this approach was neglected.

##5- Naive Bayes classifier (Amir)
By trying on on the Naive Bayes classifier, a score of `0.56603` was obtained, which was less than that obtained from the Decision Tree classifier, so this approach was neglected.

##6- Support Vector Machine classifier (Amir)
By trying on on the Support Vector Machine (SMO) classifier, a score of `0.50000` was obtained, which was less than that obtained from the Decision Tree classifier, so this approach was neglected.

##9- Neural Network with removing `SESSION_COUNT` columns (Amir)
By continuing on number `8` above (NN classifier), I removed all `SESSION_COUNT` from the prediction. The intuition behind this approach is that `USAGE` is the main concern here. However, a decreased score of `0.58118` was obtained, proving that session count indeed matters, and the approach here was neglected.

##10- Neural Network with weighted monthly usages (Amir)
By continuing on number `8` above (NN classifier), an approach was tried to weigh all monthly usages. The intuition was that the last month would be the most important month, followed by the second to last month and so on. This was implemented by multiplying the usage columns in the last 4 months by values ranging from 2 to 5.
```{r eval=FALSE}
trainDf <- trainDf %>% mutate(X207_USAGE=X207_USAGE*2, X208_USAGE=X208_USAGE*3, 
                             X209_USAGE=X209_USAGE*4,X210_USAGE=X210_USAGE*5)
testDf <- testDf %>% mutate(X207_USAGE=X207_USAGE*2, X208_USAGE=X208_USAGE*3, 
                             X209_USAGE=X209_USAGE*4,X210_USAGE=X210_USAGE*5)
```
A low score of `0.50587` was obtained as a result, so this approach was neglected. I suspect that the intuition itself could be sound but the way in which it was implemented was responsible for the decreased score.

##11- Neural Network with only last month data (Amir)
By continuing on number `8` above (NN classifier), an approach was tried to only depend on the data from the last month, and neglect all the usage and session count columns from the first four months.
```{r eval=FALSE}
trainKeeps <- c("CONTRACT_KEY","X210_SESSION_COUNT","X210_USAGE","TARGET")
trainDf <- trainDf[trainKeeps]
testKeeps <- c("CONTRACT_KEY","X210_SESSION_COUNT","X210_USAGE")
testDf <- testDf[testKeeps]
```
A score of `0.62686` was obtained which is surprisingly not much less than the original score of `0.63391`. But due to it being a lesser score, the approach was neglected.

##12- Neural Network with removing `CONTRACT_KEY` from predictions (Amir)
I realized that the `CONTRACT_KEY` in both the train and test datasets factored as an integer in the predictions, which does not make sense at all. So, by removing it from predictions, an improved score of `0.65550` was obtained with the NN classifier. So, this approach is taken into account from now on.

##13- Feature extraction to integrating roaming data (Amir)
Buidling on the approach in `12`, I integrated with the roaming data from the `roaming_monthly.csv` file. For both the train and test datasets, 10 extra features were added as the roaming usage and roaming number of sessions for each of the 5 months. The values of these columns were extracted from `roaming_monthly.csv`.
``` {r eval = FALSE}
trainRoamDf <- trainDf
trainRoamDf[,"R206_USAGE"] <- 0
trainRoamDf[,"R206_SESSION_COUNT"] <- 0
trainRoamDf[,"R207_USAGE"] <- 0
trainRoamDf[,"R207_SESSION_COUNT"] <- 0
trainRoamDf[,"R208_USAGE"] <- 0
trainRoamDf[,"R208_SESSION_COUNT"] <- 0
trainRoamDf[,"R209_USAGE"] <- 0
trainRoamDf[,"R209_SESSION_COUNT"] <- 0
trainRoamDf[,"R210_USAGE"] <- 0
trainRoamDf[,"R210_SESSION_COUNT"] <- 0

testRoamDf <- testDf
testRoamDf[,"R206_USAGE"] <- 0
testRoamDf[,"R206_SESSION_COUNT"] <- 0
testRoamDf[,"R207_USAGE"] <- 0
testRoamDf[,"R207_SESSION_COUNT"] <- 0
testRoamDf[,"R208_USAGE"] <- 0
testRoamDf[,"R208_SESSION_COUNT"] <- 0
testRoamDf[,"R209_USAGE"] <- 0
testRoamDf[,"R209_SESSION_COUNT"] <- 0
testRoamDf[,"R210_USAGE"] <- 0
testRoamDf[,"R210_SESSION_COUNT"] <- 0

for (k in unique(roamingDf$CONTRACT_KEY)) {
  orig <- roamingDf[roamingDf$CONTRACT_KEY==k,]
  if (trainRoamDf[trainRoamDf$CONTRACT_KEY==k,] %>% nrow > 0) {
    val <- orig[orig$CALL_MONTH_KEY == 206,]
    if (nrow(val) > 0) {
      trainRoamDf[trainRoamDf$CONTRACT_KEY==k,"R206_USAGE"] = val$USAGE
      trainRoamDf[trainRoamDf$CONTRACT_KEY==k,"R206_SESSION_COUNT"] = val$SESSION_COUNT
    }
    val <- orig[orig$CALL_MONTH_KEY == 207,]
    if (nrow(val) > 0) {
      trainRoamDf[trainRoamDf$CONTRACT_KEY==k,"R207_USAGE"] = val$USAGE
      trainRoamDf[trainRoamDf$CONTRACT_KEY==k,"R207_SESSION_COUNT"] = val$SESSION_COUNT
    }
    val <- orig[orig$CALL_MONTH_KEY == 208,]
    if (nrow(val) > 0) {
      trainRoamDf[trainRoamDf$CONTRACT_KEY==k,"R208_USAGE"] = val$USAGE
      trainRoamDf[trainRoamDf$CONTRACT_KEY==k,"R208_SESSION_COUNT"] = val$SESSION_COUNT
    }
    val <- val[val$CALL_MONTH_KEY == 209,]
    if (nrow(val) > 0) {
      trainRoamDf[trainRoamDf$CONTRACT_KEY==k,"R209_USAGE"] = val$USAGE
      trainRoamDf[trainRoamDf$CONTRACT_KEY==k,"R209_SESSION_COUNT"] = val$SESSION_COUNT
    }
    val <- orig[orig$CALL_MONTH_KEY == 210,]
    if (nrow(val) > 0) {
      trainRoamDf[trainRoamDf$CONTRACT_KEY==k,"R210_USAGE"] = val$USAGE
      trainRoamDf[trainRoamDf$CONTRACT_KEY==k,"R210_SESSION_COUNT"] = val$SESSION_COUNT
    }
  }
  else {
    val <- orig[orig$CALL_MONTH_KEY == 206,]
    if (nrow(val) > 0) {
      testRoamDf[testRoamDf$CONTRACT_KEY==k,"R206_USAGE"] = val$USAGE
      testRoamDf[testRoamDf$CONTRACT_KEY==k,"R206_SESSION_COUNT"] = val$SESSION_COUNT
    }
    val <- orig[orig$CALL_MONTH_KEY == 207,]
    if (nrow(val) > 0) {
      testRoamDf[testRoamDf$CONTRACT_KEY==k,"R207_USAGE"] = val$USAGE
      testRoamDf[testRoamDf$CONTRACT_KEY==k,"R207_SESSION_COUNT"] = val$SESSION_COUNT
    }
    val <- orig[orig$CALL_MONTH_KEY == 208,]
    if (nrow(val) > 0) {
      testRoamDf[testRoamDf$CONTRACT_KEY==k,"R208_USAGE"] = val$USAGE
      testRoamDf[testRoamDf$CONTRACT_KEY==k,"R208_SESSION_COUNT"] = val$SESSION_COUNT
    }
    val <- val[val$CALL_MONTH_KEY == 209,]
    if (nrow(val) > 0) {
      testRoamDf[testRoamDf$CONTRACT_KEY==k,"R209_USAGE"] = val$USAGE
      testRoamDf[testRoamDf$CONTRACT_KEY==k,"R209_SESSION_COUNT"] = val$SESSION_COUNT
    }
    val <- orig[orig$CALL_MONTH_KEY == 210,]
    if (nrow(val) > 0) {
      testRoamDf[testRoamDf$CONTRACT_KEY==k,"R210_USAGE"] = val$USAGE
      testRoamDf[testRoamDf$CONTRACT_KEY==k,"R210_SESSION_COUNT"] = val$SESSION_COUNT
    }
  }
}
```
By factoring them into the prediction, an improved score of `0.67939` was reached. So, this approach is implemented from now on.

##14- Adjustment to roaming data (Amir)
Buidling on the approach in `13`, I implemented a small adjustment by subtracting the newly added monthly roaming data from original total mothly data, so that the original usage and session count columns would now describe local data only. 
```{r eval=FALSE}
trainRoamDf <- trainRoamDf %>% mutate(X206_SESSION_COUNT = X206_SESSION_COUNT - R206_SESSION_COUNT,
                                      X206_USAGE = X206_USAGE - R206_USAGE,
                                      X207_SESSION_COUNT = X207_SESSION_COUNT - R207_SESSION_COUNT,
                                      X207_USAGE = X207_USAGE - R207_USAGE,
                                      X208_SESSION_COUNT = X208_SESSION_COUNT - R208_SESSION_COUNT,
                                      X208_USAGE = X208_USAGE - R208_USAGE,
                                      X209_SESSION_COUNT = X209_SESSION_COUNT - R209_SESSION_COUNT,
                                      X209_USAGE = X209_USAGE - R209_USAGE,
                                      X210_SESSION_COUNT = X210_SESSION_COUNT - R210_SESSION_COUNT,
                                      X210_USAGE = X210_USAGE - R210_USAGE)

testRoamDf <- testRoamDf %>% mutate(X206_SESSION_COUNT = X206_SESSION_COUNT - R206_SESSION_COUNT,
                                      X206_USAGE = X206_USAGE - R206_USAGE,
                                      X207_SESSION_COUNT = X207_SESSION_COUNT - R207_SESSION_COUNT,
                                      X207_USAGE = X207_USAGE - R207_USAGE,
                                      X208_SESSION_COUNT = X208_SESSION_COUNT - R208_SESSION_COUNT,
                                      X208_USAGE = X208_USAGE - R208_USAGE,
                                      X209_SESSION_COUNT = X209_SESSION_COUNT - R209_SESSION_COUNT,
                                      X209_USAGE = X209_USAGE - R209_USAGE,
                                      X210_SESSION_COUNT = X210_SESSION_COUNT - R210_SESSION_COUNT,
                                      X210_USAGE = X210_USAGE - R210_USAGE)
```
The score improved to `0.68188` which makes sense. So, this approach is implemented from now on.

##15- Adding `RATE_PLAN` feature (Amir)
Building on `14`, I added the `RATE_PLAN` feature from `contract_ref` in the model prediction. Building this model took several hours on my local machine, which was expected due to the large number of levels for this feature (183 levels). However, this largely sabotaged the prediction process as a minimum score of `0.50000` resulted from this approach. This indicated that some feature engineering needs to be done on this feature and all features in `contract_ref` in general to obtain more useful parameters.

##16- Adaboosting with NN as base classifier (Amir)
Buidling on `14`, instead of using NN base classifier, I used boosting with `AdaboostM1` and with NN as its base classifier. Surprisingly, I obtained a score of `0.67495` which is less than the one obtained with the base classifier only.